{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-11T13:49:44.040937Z",
     "iopub.status.busy": "2022-04-11T13:49:44.040589Z",
     "iopub.status.idle": "2022-04-11T13:49:45.516505Z",
     "shell.execute_reply": "2022-04-11T13:49:45.515769Z",
     "shell.execute_reply.started": "2022-04-11T13:49:44.040833Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:49:52.635123Z",
     "iopub.status.busy": "2022-04-11T13:49:52.634866Z",
     "iopub.status.idle": "2022-04-11T13:49:52.643240Z",
     "shell.execute_reply": "2022-04-11T13:49:52.642185Z",
     "shell.execute_reply.started": "2022-04-11T13:49:52.635096Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "hashtag_regex = r\"#(\\w+)\"\n",
    "mention_regex = r\"\\B@(?!(?:[a-z0-9.]*_){2})(?!(?:[a-z0-9_]*\\.){2})[._a-z0-9]{3,24}\\b\"\n",
    "start = ['<pad>','<pad>','<pad>','<pad>']\n",
    "# end = np.array(['<pad>','<pad>','<pad>'])\n",
    "punc = [',', '.', '!', ';', '?']\n",
    "def tokenizer(messages):\n",
    "    tokenized_data = []\n",
    "#     for messages in data:\n",
    "    messages = messages.lower()\n",
    "#         messages = re.sub(url_regex,'<URL>',messages)\n",
    "#         messages = re.sub(hashtag_regex,'<HASHTAG>',messages)\n",
    "#         messages = re.sub(mention_regex, '<MENTION>',messages)\n",
    "#         newmessage = ''\n",
    "#         for i in range(len(messages)):\n",
    "#             if re.match(r'[^\\w\\s]', messages[i]) and messages[i]!='<' and messages[i]!='>':\n",
    "#                 newmessage+=' '+messages[i]+' '\n",
    "#             else:\n",
    "#                 newmessage+=messages[i]\n",
    "\n",
    "#         if newmessage == '':\n",
    "#             continue\n",
    "    messages = re.sub(r'[^\\w\\s]','',messages)\n",
    "    words = re.split('\\s|\\t|\\n', messages)\n",
    "    nowo = []\n",
    "    for i in words:\n",
    "        if i!='':\n",
    "            nowo.append(i)\n",
    "#         if len(words)==0:\n",
    "#             continue\n",
    "#         m = len(words)\n",
    "#         for i  in range(m):\n",
    "#             if len(words[i])!=1:\n",
    "#                 continue\n",
    "#             if words[i] in punc and words[i] not in ['s','e','<URL>','<HASHTAG>','<MENTION>']:\n",
    "#                 words[i] = re.sub(r'[^\\w\\s]','',words[i])\n",
    "#         words = words[words!='']\n",
    "#         for wo in words:\n",
    "#             tokenized_data.append(wo)\n",
    "        #tokenized_data.append(np.concatenate((start, words, end)))\n",
    "    return nowo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:00.671473Z",
     "iopub.status.busy": "2022-04-11T13:50:00.670955Z",
     "iopub.status.idle": "2022-04-11T13:50:00.692142Z",
     "shell.execute_reply": "2022-04-11T13:50:00.690682Z",
     "shell.execute_reply.started": "2022-04-11T13:50:00.671441Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    \"\"\" Function to read input data\n",
    "    Args:\n",
    "        path (string): the parent path of the folder containing the input text files\n",
    "    Returns:\n",
    "        string: The complete text read from input files appended in a single string.\n",
    "    \"\"\"\n",
    "    text = ' '\n",
    "    f = open(path, 'r')\n",
    "    text = f.readlines()\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Function for basic cleaning and pre-processing of input text\n",
    "    Args:\n",
    "        text (string): raw input text\n",
    "    Returns:\n",
    "        string: cleaned text\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for line in text:\n",
    "        data.append(tokenizer(line))\n",
    "    return data\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"'s\\b\", \"\", text)\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     text = ' '.join([word for word in text.split() if len(word) >= 3]).strip()\n",
    "\n",
    "#     return text\n",
    "\n",
    "\n",
    "def prepare_text(data, n, word_id):\n",
    "    \"\"\" Function to prepare text in sequence of ngrams\n",
    "    Args:\n",
    "        text (string): complete input text\n",
    "    Returns:\n",
    "        list : a list of text sequence with 31 characters each\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        x = len(data[i])\n",
    "        gram = []\n",
    "        gram.append(2)\n",
    "        for j in range(x):\n",
    "            gram.append(word_id[data[i][j]])\n",
    "        for j in range(n-x-2):\n",
    "            gram.append(1)\n",
    "        gram.append(3)\n",
    "        new_data.append(gram)\n",
    "    return new_data\n",
    "\n",
    "def create_data(data, word_id, id_word):\n",
    "    \"\"\" Function to encode the character sequence into number sequence\n",
    "    Args:\n",
    "        text (string): cleaned text\n",
    "        sequence (list): character sequence list\n",
    "    Returns:\n",
    "        dict: dictionary mapping of all unique input charcters to integers\n",
    "        list: number encoded charachter sequences\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in data:\n",
    "        inp = []\n",
    "        for j in range(4):\n",
    "            if i[j] not in word_id.keys():\n",
    "                inp.append(word_id['<UNKNOWN>'])\n",
    "            else:\n",
    "                inp.append(word_id[i[j]])\n",
    "        if i[4] not in word_id.keys():\n",
    "            y.append(word_id['<UNKNOWN>'])\n",
    "        else:\n",
    "            y.append(word_id[i[4]])\n",
    "        x.append(inp)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def split_data(mapping, encoded_sequence):\n",
    "    \"\"\" Function to split the prepared data in train and test\n",
    "    Args:\n",
    "        mapping (dict): dictionary mapping of all unique input charcters to integers\n",
    "        encoded_sequence (list): number encoded charachter sequences\n",
    "    Returns:\n",
    "        numpy array : train and test split numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_sequence_ = np.array(encoded_sequence)\n",
    "    X, y = encoded_sequence_[:, :-1], encoded_sequence_[:, -1]\n",
    "    y = to_categorical(y, num_classes=len(mapping))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_vocab(data, threshold =  5):\n",
    "    freq = {}\n",
    "    for j in data:\n",
    "        for i in j:\n",
    "            if i not in freq.keys():\n",
    "                freq[i] = 0\n",
    "            freq[i]+=1\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if freq[data[i][j]] <= threshold:\n",
    "                data[i][j] = '<UNKNOWN>'\n",
    "    word_id = {}\n",
    "    id_word = {}\n",
    "    word_id['<UNKNOWN>'] = 0\n",
    "    word_id['<START>'] = 2\n",
    "    word_id['<END>'] = 3\n",
    "    word_id['<PAD>'] = 1\n",
    "    id_word[0] = '<UNKNOWN>'\n",
    "    id_word[1] = '<PAD>'\n",
    "    id_word[2] = '<START>'\n",
    "    id_word[3] = '<END>'\n",
    "    cur = 4\n",
    "    for j in data:\n",
    "        for i in j:\n",
    "            if i not in word_id.keys():\n",
    "                word_id[i] = cur\n",
    "                id_word[cur] = i\n",
    "                cur+=1\n",
    "    return data, word_id, id_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:10.570611Z",
     "iopub.status.busy": "2022-04-11T13:50:10.570357Z",
     "iopub.status.idle": "2022-04-11T13:50:10.667062Z",
     "shell.execute_reply": "2022-04-11T13:50:10.666311Z",
     "shell.execute_reply.started": "2022-04-11T13:50:10.570582Z"
    }
   },
   "outputs": [],
   "source": [
    "text_en = read_text('./dataset/ted-talks-corpus/train.en')\n",
    "text_fr = read_text('./dataset/ted-talks-corpus/train.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:12.380521Z",
     "iopub.status.busy": "2022-04-11T13:50:12.379972Z",
     "iopub.status.idle": "2022-04-11T13:50:12.410792Z",
     "shell.execute_reply": "2022-04-11T13:50:12.410107Z",
     "shell.execute_reply.started": "2022-04-11T13:50:12.380483Z"
    }
   },
   "outputs": [],
   "source": [
    "# text_en_test = read_text('./dataset/ted-talks-corpus/test.en')\n",
    "# text_fr_test = read_text('./dataset/ted-talks-corpus/test.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:14.500537Z",
     "iopub.status.busy": "2022-04-11T13:50:14.500015Z",
     "iopub.status.idle": "2022-04-11T13:50:15.845628Z",
     "shell.execute_reply": "2022-04-11T13:50:15.844916Z",
     "shell.execute_reply.started": "2022-04-11T13:50:14.500503Z"
    }
   },
   "outputs": [],
   "source": [
    "data_en = preprocess_text(text_en)\n",
    "data_fr = preprocess_text(text_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:17.579642Z",
     "iopub.status.busy": "2022-04-11T13:50:17.579393Z",
     "iopub.status.idle": "2022-04-11T13:50:17.611793Z",
     "shell.execute_reply": "2022-04-11T13:50:17.611003Z",
     "shell.execute_reply.started": "2022-04-11T13:50:17.579612Z"
    }
   },
   "outputs": [],
   "source": [
    "mx_en = 40\n",
    "mx_fr = 40\n",
    "co = np.zeros(500)\n",
    "co1 = np.zeros(500)\n",
    "da_en = []\n",
    "da_fr = []\n",
    "sen_id = []\n",
    "for i in range(len(data_en)):\n",
    "    if len(data_en[i])<=38 and len(data_fr[i])<=38:\n",
    "        da_en.append(data_en[i])\n",
    "        da_fr.append(data_fr[i])\n",
    "        sen_id.append(i)\n",
    "len(da_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:19.220313Z",
     "iopub.status.busy": "2022-04-11T13:50:19.219864Z",
     "iopub.status.idle": "2022-04-11T13:50:19.753874Z",
     "shell.execute_reply": "2022-04-11T13:50:19.753123Z",
     "shell.execute_reply.started": "2022-04-11T13:50:19.220276Z"
    }
   },
   "outputs": [],
   "source": [
    "da_en , word_id_en , id_word_en = create_vocab(da_en,10)\n",
    "da_fr , word_id_fr , id_word_fr = create_vocab(da_fr,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:20.780487Z",
     "iopub.status.busy": "2022-04-11T13:50:20.779662Z",
     "iopub.status.idle": "2022-04-11T13:50:21.423984Z",
     "shell.execute_reply": "2022-04-11T13:50:21.423176Z",
     "shell.execute_reply.started": "2022-04-11T13:50:20.780442Z"
    }
   },
   "outputs": [],
   "source": [
    "en_seq = np.array(prepare_text(da_en, mx_en, word_id_en))\n",
    "fr_seq = np.array(prepare_text(da_fr, mx_fr, word_id_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:22.821102Z",
     "iopub.status.busy": "2022-04-11T13:50:22.820294Z",
     "iopub.status.idle": "2022-04-11T13:50:22.874314Z",
     "shell.execute_reply": "2022-04-11T13:50:22.873590Z",
     "shell.execute_reply.started": "2022-04-11T13:50:22.821059Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_test_en = preprocess_text(text_en_test)\n",
    "# data_test_fr = preprocess_text(text_fr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:25.210263Z",
     "iopub.status.busy": "2022-04-11T13:50:25.210011Z",
     "iopub.status.idle": "2022-04-11T13:50:25.413353Z",
     "shell.execute_reply": "2022-04-11T13:50:25.412621Z",
     "shell.execute_reply.started": "2022-04-11T13:50:25.210234Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_test(line, word_id):\n",
    "    mess = []\n",
    "    for wo in line:\n",
    "        if wo in word_id.keys():\n",
    "            mess.append(wo)\n",
    "        else:\n",
    "            mess.append('<UNKNOWN>')\n",
    "    return mess\n",
    "        \n",
    "sen_id_test = []\n",
    "test_data = []\n",
    "da_en_test = []\n",
    "da_fr_test = []\n",
    "for i in range(len(text_en_test)):\n",
    "    if len(data_test_en[i])<=38 and len(data_test_fr[i])<=38:\n",
    "        sen_id_test.append(i)\n",
    "        da_en_test.append(prepare_test(data_test_en[i], word_id_en))\n",
    "        da_fr_test.append(prepare_test(data_test_fr[i], word_id_fr))\n",
    "en = np.array(prepare_text(da_en_test, 40, word_id_en))\n",
    "fr = np.array(prepare_text(da_fr_test, 40, word_id_fr))\n",
    "\n",
    "for i in range(len(da_en_test)):\n",
    "    test_data.append((en[i], fr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:29.430298Z",
     "iopub.status.busy": "2022-04-11T13:50:29.430039Z",
     "iopub.status.idle": "2022-04-11T13:50:29.435560Z",
     "shell.execute_reply": "2022-04-11T13:50:29.434891Z",
     "shell.execute_reply.started": "2022-04-11T13:50:29.430269Z"
    }
   },
   "outputs": [],
   "source": [
    "en_vocab_size = len(word_id_en)\n",
    "fr_vocab_size = len(word_id_fr)\n",
    "embedding_size = 50\n",
    "en_vocab_size, fr_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:32.500533Z",
     "iopub.status.busy": "2022-04-11T13:50:32.499921Z",
     "iopub.status.idle": "2022-04-11T13:50:32.534392Z",
     "shell.execute_reply": "2022-04-11T13:50:32.533659Z",
     "shell.execute_reply.started": "2022-04-11T13:50:32.500498Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(en_seq)):\n",
    "    train_data.append((en_seq[i],fr_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:35.160130Z",
     "iopub.status.busy": "2022-04-11T13:50:35.159731Z",
     "iopub.status.idle": "2022-04-11T13:50:37.026881Z",
     "shell.execute_reply": "2022-04-11T13:50:37.026034Z",
     "shell.execute_reply.started": "2022-04-11T13:50:35.160097Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:37.028924Z",
     "iopub.status.busy": "2022-04-11T13:50:37.028589Z",
     "iopub.status.idle": "2022-04-11T13:50:37.081898Z",
     "shell.execute_reply": "2022-04-11T13:50:37.081071Z",
     "shell.execute_reply.started": "2022-04-11T13:50:37.028889Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:37.585556Z",
     "iopub.status.busy": "2022-04-11T13:50:37.585141Z",
     "iopub.status.idle": "2022-04-11T13:50:37.603001Z",
     "shell.execute_reply": "2022-04-11T13:50:37.602064Z",
     "shell.execute_reply.started": "2022-04-11T13:50:37.585514Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "#         init.normal_(self.embedding.weight, 0.0, 0.2)\n",
    "        self.si2 = self.hidden_size//2\n",
    "        print(self.si2)\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_size,hidden_size=self.si2,num_layers=self.n_layers,batch_first=True,bidirectional=True)\n",
    "\n",
    "    def forward(self, word_inputs, hidden):         \n",
    "        embedded = self.embedding(word_inputs)\n",
    "        \n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batches):\n",
    "        hidden = (torch.randn(2*self.n_layers, batches, self.hidden_size//2).to(device),torch.randn(2*self.n_layers, batches, self.hidden_size//2).to(device))\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "#         init.normal_(self.embedding.weight, 0.0, 0.2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.hidden_size,hidden_size=self.hidden_size,num_layers=self.n_layers,batch_first=True,bidirectional=False)\n",
    "\n",
    "    def forward(self, word_inputs, hidden):\n",
    "        embedded = self.embedding(word_inputs).unsqueeze_(1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:41.060997Z",
     "iopub.status.busy": "2022-04-11T13:50:41.060718Z",
     "iopub.status.idle": "2022-04-11T13:50:41.077560Z",
     "shell.execute_reply": "2022-04-11T13:50:41.076688Z",
     "shell.execute_reply.started": "2022-04-11T13:50:41.060966Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, n_layers):\n",
    "        super(Seq2seq, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = EncoderRNN(input_vocab_size, hidden_size, self.n_layers)\n",
    "        self.decoder = DecoderRNN(output_vocab_size, hidden_size, self.n_layers)\n",
    "\n",
    "        self.W = nn.Linear(hidden_size, output_vocab_size)\n",
    "#         init.normal_(self.W.weight, 0.0, 0.2)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def _forward_encoder(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        init_hidden = self.encoder.init_hidden(batch_size)\n",
    "        encoder_outputs, encoder_hidden = self.encoder(x, init_hidden)\n",
    "        encoder_hidden_h, encoder_hidden_c = encoder_hidden\n",
    "\n",
    "        self.decoder_hidden_h = encoder_hidden_h.permute(1,0,2).reshape(batch_size, self.n_layers, self.hidden_size).permute(1,0,2)\n",
    "        self.decoder_hidden_c = encoder_hidden_c.permute(1,0,2).reshape(batch_size, self.n_layers, self.hidden_size).permute(1,0,2)\n",
    "        return self.decoder_hidden_h, self.decoder_hidden_c\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        decoder_hidden_h, decoder_hidden_c = self._forward_encoder(x)\n",
    "\n",
    "        H = []\n",
    "        for i in range(y.shape[1]):\n",
    "            input = y[:, i]\n",
    "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "            # h: (batch_size, vocab_size)\n",
    "            h = self.W(decoder_output.squeeze(1))\n",
    "            # h: (batch_size, vocab_size, 1)\n",
    "            H.append(h.unsqueeze(2))\n",
    "\n",
    "        # H: (batch_size, vocab_size, seq_len)\n",
    "        return torch.cat(H, dim=2)\n",
    "    \n",
    "    def forward2(self, x):\n",
    "        decoder_hidden_h, decoder_hidden_c = self._forward_encoder(x)\n",
    "\n",
    "        current_y = 2\n",
    "        result = []\n",
    "        counter = 0\n",
    "        while current_y!=3 and counter < 40:\n",
    "            input = torch.tensor([current_y]).to(device)\n",
    "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "            # h: (vocab_size)\n",
    "            h = self.W(decoder_output.squeeze(1)).squeeze(0)\n",
    "#             print(h.shape)\n",
    "            y = F.softmax(h, dim = 0)\n",
    "            _, current_y = torch.max(y, dim=0)\n",
    "            current_y = current_y.item()\n",
    "#             print(current_y)\n",
    "            if current_y != 1:\n",
    "                result.append(id_word_fr[current_y])\n",
    "            counter += 1\n",
    "\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:44.439782Z",
     "iopub.status.busy": "2022-04-11T13:50:44.439407Z",
     "iopub.status.idle": "2022-04-11T13:50:44.448887Z",
     "shell.execute_reply": "2022-04-11T13:50:44.448218Z",
     "shell.execute_reply.started": "2022-04-11T13:50:44.439749Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def bleuScore(model, train_data, batch=1):\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch, shuffle=False) \n",
    "    scores = []\n",
    "    translations = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            sen2 = model.forward2(imgs.to(device))\n",
    "            translations.append(sen2)\n",
    "            sen1 = da_fr[count]\n",
    "            count+=1\n",
    "            score= 0.0\n",
    "            if len(sen1)<4 or len(sen2)<4:\n",
    "                score = sentence_bleu([sen1],sen2,weights=(0.5,0.5))\n",
    "            else:\n",
    "                score = sentence_bleu([sen1],sen2)\n",
    "            scores.append(score)\n",
    "#             print(sen1, sen2, score)\n",
    "    return scores, translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:46.535623Z",
     "iopub.status.busy": "2022-04-11T13:50:46.535374Z",
     "iopub.status.idle": "2022-04-11T13:50:46.546600Z",
     "shell.execute_reply": "2022-04-11T13:50:46.545613Z",
     "shell.execute_reply.started": "2022-04-11T13:50:46.535596Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def trainvalAdam(model, train_data, device, batch_size=100, num_iters=1, learn_rate=0.01):\n",
    "    yo = train_data.copy()\n",
    "    random.shuffle(yo)\n",
    "    train_loader = torch.utils.data.DataLoader(yo[:25000], batch_size=batch_size, shuffle=True) # shuffle after every epoch\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learn_rate, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.90, 0.98), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    # optimizer = optim.Adadelta(model.parameters(),lr=1.0, rho=0.95, eps=1e-08, weight_decay=0.0)\n",
    "    # training\n",
    "    losses = []\n",
    "    avg_scores = []\n",
    "    prev = 0\n",
    "    best_score = -1e9\n",
    "    si = len(train_data)\n",
    "    n = 0 # the number of iterations\n",
    "    for n in tqdm(range(num_iters)):\n",
    "        tot_loss = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            # optimizer.zero_grad()         # a clean up step for PyTorch\n",
    "            out = model.forward(imgs,labels)             # forward pass\n",
    "#             print(out.shape)\n",
    "#             print(labels.shape)\n",
    "            loss = criterion(out[:,:,1:39], labels[:,1:39]) # compute the total loss\n",
    "#             print(loss)\n",
    "            tot_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()               # backward pass (compute parameter updates)\n",
    "            optimizer.step()  # make the updates for each parameter\n",
    "        losses.append(tot_loss/si)\n",
    "        scores= np.array(bleuScore(model , yo[25000:]))\n",
    "        score = scores.sum()/len(scores)\n",
    "        avg_scores.append(score)\n",
    "        print('epoch '+str(n)+' = '+str(tot_loss/si)+'  and  score is: '+str(score))\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            prev = n\n",
    "        if n-prev>10:\n",
    "            break\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T13:33:23.369624Z",
     "iopub.status.busy": "2022-04-10T13:33:23.369338Z",
     "iopub.status.idle": "2022-04-10T13:45:15.099887Z",
     "shell.execute_reply": "2022-04-10T13:45:15.099166Z",
     "shell.execute_reply.started": "2022-04-10T13:33:23.369592Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = Seq2seq(en_vocab_size, fr_vocab_size, embedding_size, 1)\n",
    "# print(model)\n",
    "# # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# # device = \"cpu\"\n",
    "# print(device)\n",
    "# model.to(device)\n",
    "# train_losses = trainvalAdam(model, train_data, device, num_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T13:55:07.296624Z",
     "iopub.status.busy": "2022-04-10T13:55:07.295626Z",
     "iopub.status.idle": "2022-04-10T13:55:07.569184Z",
     "shell.execute_reply": "2022-04-10T13:55:07.56851Z",
     "shell.execute_reply.started": "2022-04-10T13:55:07.296579Z"
    }
   },
   "outputs": [],
   "source": [
    "# ep = [i for i in range(1,14)]\n",
    "# loss = []\n",
    "# for i in train_losses:\n",
    "#     loss.append(float(i))\n",
    "# plt.plot(ep,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T07:19:22.994483Z",
     "iopub.status.busy": "2022-04-11T07:19:22.994206Z",
     "iopub.status.idle": "2022-04-11T07:19:23.150007Z",
     "shell.execute_reply": "2022-04-11T07:19:23.149018Z",
     "shell.execute_reply.started": "2022-04-11T07:19:22.994449Z"
    }
   },
   "outputs": [],
   "source": [
    "# score = [0.01914861144514335,  0.12519344714329764,  0.12519344714329764,0.12519344714329764,0.12503745053114246,0.12266798567987887, 0.12266798567987887,0.12251198906772368, 0.12251198906772368, 0.12251198906772368, 0.12251198906772368, 0.12251198906772368, 0.12251198906772368]\n",
    "# plt.plot(ep, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:50:52.621529Z",
     "iopub.status.busy": "2022-04-11T13:50:52.620885Z",
     "iopub.status.idle": "2022-04-11T13:50:56.483648Z",
     "shell.execute_reply": "2022-04-11T13:50:56.482893Z",
     "shell.execute_reply.started": "2022-04-11T13:50:52.621497Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Seq2seq(en_vocab_size, fr_vocab_size, embedding_size, 1)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./models/model_translate_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T13:51:54.346356Z",
     "iopub.status.busy": "2022-04-10T13:51:54.346023Z",
     "iopub.status.idle": "2022-04-10T13:51:54.364392Z",
     "shell.execute_reply": "2022-04-10T13:51:54.363436Z",
     "shell.execute_reply.started": "2022-04-10T13:51:54.346318Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),'./model_translate_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:51:08.371475Z",
     "iopub.status.busy": "2022-04-11T13:51:08.371096Z",
     "iopub.status.idle": "2022-04-11T13:51:28.592096Z",
     "shell.execute_reply": "2022-04-11T13:51:28.591263Z",
     "shell.execute_reply.started": "2022-04-11T13:51:08.371416Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_scores = bleuScore(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T13:51:48.261641Z",
     "iopub.status.busy": "2022-04-11T13:51:48.261361Z",
     "iopub.status.idle": "2022-04-11T13:58:59.117297Z",
     "shell.execute_reply": "2022-04-11T13:58:59.116563Z",
     "shell.execute_reply.started": "2022-04-11T13:51:48.261611Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_scores = bleuScore(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T14:11:07.896002Z",
     "iopub.status.busy": "2022-04-10T14:11:07.895731Z",
     "iopub.status.idle": "2022-04-10T14:11:07.949085Z",
     "shell.execute_reply": "2022-04-10T14:11:07.948451Z",
     "shell.execute_reply.started": "2022-04-10T14:11:07.895971Z"
    }
   },
   "outputs": [],
   "source": [
    "# avg_score = np.array(train_scores).sum()/len(train_scores)\n",
    "# f = open('train-translation.txt','w')\n",
    "# f.write('Average bleu score = '+str(avg_score)+'\\n')\n",
    "# for i in range(len(sen_id)):\n",
    "#     f.write(text_en[sen_id[i]]+'     '+str(float(train_scores[i]))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-11T08:42:16.684706Z",
     "iopub.status.busy": "2022-04-11T08:42:16.68442Z",
     "iopub.status.idle": "2022-04-11T08:42:16.693651Z",
     "shell.execute_reply": "2022-04-11T08:42:16.692942Z",
     "shell.execute_reply.started": "2022-04-11T08:42:16.684676Z"
    }
   },
   "outputs": [],
   "source": [
    "# avg_score = np.array(test_scores).sum()/len(test_scores)\n",
    "# f = open('test-translation.txt','w')\n",
    "# f.write('Average bleu score = '+str(avg_score)+'\\n')\n",
    "# for i in range(len(sen_id_test)):\n",
    "#     f.write(text_en_test[sen_id_test[i]]+'     '+str(float(test_scores[i]))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = input('Give english sentence as input: \\n')\n",
    "text_en_test = [sent]\n",
    "data_test_en = preprocess_text(text_en_test)\n",
    "\n",
    "def prepare_test(line, word_id):\n",
    "    mess = []\n",
    "    for wo in line:\n",
    "        if wo in word_id.keys():\n",
    "            mess.append(wo)\n",
    "        else:\n",
    "            mess.append('<UNKNOWN>')\n",
    "    return mess\n",
    "        \n",
    "da_en_test = []\n",
    "for i in range(len(text_en_test)):\n",
    "    if len(data_test_en[i])<=38 and len(data_test_fr[i])<=38:\n",
    "        da_en_test.append(prepare_test(data_test_en[i], word_id_en))\n",
    "        \n",
    "en = torch.tensor(prepare_text(da_en_test, 40, word_id_en)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    sen = model.forward2(en[0])\n",
    "    print('output translation is: \\n',sen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
