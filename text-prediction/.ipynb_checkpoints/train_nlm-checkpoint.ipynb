{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "hashtag_regex = r\"#(\\w+)\"\n",
    "mention_regex = r\"\\B@(?!(?:[a-z0-9.]*_){2})(?!(?:[a-z0-9_]*\\.){2})[._a-z0-9]{3,24}\\b\"\n",
    "\n",
    "start = np.array(['<s>','<s>','<s>'])\n",
    "end = np.array(['<e>','<e>','<e>'])\n",
    "punc = [',', '.', '!', ';', '?']\n",
    "def tokenizer(messages):\n",
    "    tokenized_data = []\n",
    "#     for messages in data:\n",
    "    messages = messages.lower()\n",
    "#         messages = re.sub(url_regex,'<URL>',messages)\n",
    "#         messages = re.sub(hashtag_regex,'<HASHTAG>',messages)\n",
    "#         messages = re.sub(mention_regex, '<MENTION>',messages)\n",
    "#         newmessage = ''\n",
    "#         for i in range(len(messages)):\n",
    "#             if re.match(r'[^\\w\\s]', messages[i]) and messages[i]!='<' and messages[i]!='>':\n",
    "#                 newmessage+=' '+messages[i]+' '\n",
    "#             else:\n",
    "#                 newmessage+=messages[i]\n",
    "\n",
    "#         if newmessage == '':\n",
    "#             continue\n",
    "    messages = re.sub(r'[^\\w\\s]','',messages)\n",
    "    words = re.split('\\s|\\t|\\n', messages)\n",
    "#         words = words[words!='']\n",
    "#         if len(words)==0:\n",
    "#             continue\n",
    "#         m = len(words)\n",
    "#         for i  in range(m):\n",
    "#             if len(words[i])!=1:\n",
    "#                 continue\n",
    "#             if words[i] in punc and words[i] not in ['s','e','<URL>','<HASHTAG>','<MENTION>']:\n",
    "#                 words[i] = re.sub(r'[^\\w\\s]','',words[i])\n",
    "#         words = words[words!='']\n",
    "#         for wo in words:\n",
    "#             tokenized_data.append(wo)\n",
    "        #tokenized_data.append(np.concatenate((start, words, end)))\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    df = []\n",
    "    si = 0\n",
    "    tot = 0\n",
    "    with open(path) as infile:\n",
    "        for ob in infile:\n",
    "            flag = rand.randint(0,100)\n",
    "            tot+=1\n",
    "            if flag>2:\n",
    "                continue\n",
    "            si+=1\n",
    "            # if si%10000 == 0:\n",
    "            #     print(si,tot)\n",
    "            obj = json.loads(ob)\n",
    "            cur = obj['reviewText']\n",
    "            df.append(tokenizer(cur))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    \"\"\" Function to read input data\n",
    "    Args:\n",
    "        path (string): the parent path of the folder containing the input text files\n",
    "    Returns:\n",
    "        string: The complete text read from input files appended in a single string.\n",
    "    \"\"\"\n",
    "    text = ' '\n",
    "    f = open(path, 'r')\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Function for basic cleaning and pre-processing of input text\n",
    "    Args:\n",
    "        text (string): raw input text\n",
    "    Returns:\n",
    "        string: cleaned text\n",
    "    \"\"\"\n",
    "    data = tokenizer(text)\n",
    "    return data\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r\"'s\\b\", \"\", text)\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     text = ' '.join([word for word in text.split() if len(word) >= 3]).strip()\n",
    "\n",
    "#     return text\n",
    "\n",
    "\n",
    "def prepare_text(data, n):\n",
    "    \"\"\" Function to prepare text in sequence of ngrams\n",
    "    Args:\n",
    "        text (string): complete input text\n",
    "    Returns:\n",
    "        list : a list of text sequence with 31 characters each\n",
    "    \"\"\"\n",
    "    n_grams = []\n",
    "    for i in range(len(data)-n):\n",
    "        n_grams.append(data[i:i+n])\n",
    "    return n_grams\n",
    "\n",
    "def create_data(data, word_id, id_word):\n",
    "    \"\"\" Function to encode the character sequence into number sequence\n",
    "    Args:\n",
    "        text (string): cleaned text\n",
    "        sequence (list): character sequence list\n",
    "    Returns:\n",
    "        dict: dictionary mapping of all unique input charcters to integers\n",
    "        list: number encoded charachter sequences\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in data:\n",
    "        inp = []\n",
    "        for j in range(4):\n",
    "            inp.append(word_id[i[j]])\n",
    "        y.append(word_id[i[4]])\n",
    "        x.append(inp)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def split_data(mapping, encoded_sequence):\n",
    "    \"\"\" Function to split the prepared data in train and test\n",
    "    Args:\n",
    "        mapping (dict): dictionary mapping of all unique input charcters to integers\n",
    "        encoded_sequence (list): number encoded charachter sequences\n",
    "    Returns:\n",
    "        numpy array : train and test split numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_sequence_ = np.array(encoded_sequence)\n",
    "    X, y = encoded_sequence_[:, :-1], encoded_sequence_[:, -1]\n",
    "    y = to_categorical(y, num_classes=len(mapping))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_vocab(data, threshold =  5):\n",
    "    freq = {}\n",
    "    for i in data:\n",
    "        if i not in freq.keys():\n",
    "            freq[i] = 0\n",
    "        freq[i]+=1\n",
    "    for i in range(len(data)):\n",
    "        if freq[data[i]] <= threshold:\n",
    "            data[i] = '<UNKNOWN>'\n",
    "    word_id = {}\n",
    "    id_word = {}\n",
    "    cur = 0\n",
    "    for i in data:\n",
    "        if i not in word_id.keys():\n",
    "            word_id[i] = cur\n",
    "            id_word[cur] = i\n",
    "            cur+=1\n",
    "    return data, word_id, id_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text('./dataset/europarl-corpus/train.europarl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, word_id, id_word = create_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = prepare_text(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_data(n_grams, word_id, id_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/mr-anon/.local/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-16854a88a668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# See Note [Global dependencies]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/mr-anon/.local/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
